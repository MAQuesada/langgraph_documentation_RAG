{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a1af60d",
      "metadata": {},
      "source": [
        "## Setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "822d5262",
      "metadata": {},
      "outputs": [],
      "source": [
        "from vector_database.src.text_splitter import chunk_documents, save_chunks_to_disk\n",
        "from vector_database.src.utils import load_config\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Load environment\n",
        "load_dotenv()\n",
        "\n",
        "# 2. Load config\n",
        "config_path = Path(\"config.yaml\")\n",
        "config = load_config(config_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9b8825",
      "metadata": {},
      "source": [
        "## Download documents only if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bf1adc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning from https://github.com/langchain-ai/langgraph.git to /Users/manuelalejandroquesada/PERSONAL/REPOSITORIES/langgraph_documentation_RAG/docs/source_docs...\n"
          ]
        }
      ],
      "source": [
        "from vector_database.src.documentation_loader import clone_repo, cleanup_old_outputs\n",
        "\n",
        "# 3. If you want to download the docs from GitHub again, run this:\n",
        "cleanup_old_outputs()\n",
        "clone_repo(config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7dff681",
      "metadata": {},
      "source": [
        "## Load the Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64b7965",
      "metadata": {},
      "outputs": [],
      "source": [
        "from vector_database.src.documentation_loader import load_documents\n",
        "\n",
        "# 4. Load documents\n",
        "docs_path = config['data_source']['github']['target_path']\n",
        "all_docs = load_documents(docs_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f76b5e4",
      "metadata": {},
      "source": [
        "## Split documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a230ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 5. Chunk and save\n",
        "chunks = chunk_documents(all_docs, config)\n",
        "save_chunks_to_disk(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab1f94e",
      "metadata": {},
      "source": [
        "## Populate Qdrant Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be34645",
      "metadata": {},
      "outputs": [],
      "source": [
        "from vector_database.src.vector_store import store_embeddings\n",
        "\n",
        "# 6. Store embeddings to Qdrant\n",
        "store_embeddings(chunks, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead83bce",
      "metadata": {},
      "source": [
        "## LangChain vector database  to use in RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b37a26c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "from vector_database.src.vector_store import COLLECTION_NAME,client,embeddings\n",
        "\n",
        "\n",
        "# client.create_collection(\n",
        "#     collection_name=COLLECTION_NAME,\n",
        "#     vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
        "# )\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e47d4f9f",
      "metadata": {},
      "source": [
        "## Initializate the RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4bc5660",
      "metadata": {},
      "outputs": [],
      "source": [
        "from rag_pipeline.core import RAGPipeline\n",
        "from prompts.core import PromptBuilder\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "builder = PromptBuilder(config_path=\"prompts/config.yaml\")\n",
        "\n",
        "rag_config = {\"configurable\": {\"thread_id\": \"test\"}}\n",
        "\n",
        "topic_guard_prompt, _ = builder.build_prompt(\n",
        "    file_path=\"../prompts/topic_guard.yml\")\n",
        "rag_system_prompt, _ = builder.build_prompt(\n",
        "    file_path=\"../prompts/rag_system_prompt.yml\")\n",
        "\n",
        "checkpointer = MemorySaver()  # only for testing, for production use `PostgresSaver`\n",
        "\n",
        "\n",
        "rag = RAGPipeline(\n",
        "    checkpoint=checkpointer,\n",
        "    vectorstore=vector_store,\n",
        "    topic_guard_prompt=topic_guard_prompt,\n",
        "    rag_system_prompt=rag_system_prompt,\n",
        "    llm_temperature=0.1,\n",
        "    llm_model_name=\"gpt-4o-mini\",\n",
        "    num_history_messages=5,\n",
        "    num_retrieval_chunks=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46818940",
      "metadata": {},
      "source": [
        "## Chat Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e4d5d518",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/wf/r36br2vs18bdnw4y1vjw_7sc0000gn/T/ipykernel_45912/2027090792.py:34: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
            "  input_box.on_submit(on_submit_enter)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a510c7f4a3a46b7835359ac6d422406",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', description='User:', layout=Layout(width='100%'), placeholder='Type your question and press Entâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f87a80c064044eeb7a981431fcbed8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# Input widget\n",
        "input_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your question and press Enter...',\n",
        "    description='User:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Output widget\n",
        "output_box = widgets.Output()\n",
        "\n",
        "# Handler for Enter key\n",
        "\n",
        "\n",
        "def on_submit_enter(text):\n",
        "    question = input_box.value\n",
        "    answer, sources = rag.chat(question, config=rag_config)\n",
        "    with output_box:\n",
        "        display(Markdown(f\"**ðŸ§‘ User:** {question}\"))\n",
        "        if sources:\n",
        "            sources = \" -- \".join(sources)\n",
        "            display(Markdown(f\"**ðŸ“š Sources:** {sources}\"))\n",
        "        # This will render the bot's response as Markdown\n",
        "        display(Markdown(f\"**ðŸ¤– Bot:**\\n {answer}\"))\n",
        "        display(Markdown(\"---\"))\n",
        "    input_box.value = ''\n",
        "\n",
        "\n",
        "# Trigger on Enter\n",
        "input_box.on_submit(on_submit_enter)\n",
        "\n",
        "# Show interface\n",
        "display(input_box)\n",
        "display(output_box)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
