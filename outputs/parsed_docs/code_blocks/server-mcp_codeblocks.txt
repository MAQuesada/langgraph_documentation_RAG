Language: 
/mcp

---

Language: bash
pip install "langgraph-api>=0.2.3" "langgraph-sdk>=0.1.61"

---

Language: json
{
    "graphs": {
        "my_agent": {
            "path": "./my_agent/agent.py:graph",
            "description": "A description of what the agent does"
        }
    },
    "env": ".env"
}

---

Language: python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

# Define input schema
class InputState(TypedDict):
    question: str

# Define output schema
class OutputState(TypedDict):
    answer: str

# Combine input and output
class OverallState(InputState, OutputState):
    pass

# Define the processing node
def answer_node(state: InputState):
    # Replace with actual logic and do something useful
    return {"answer": "bye", "question": state["question"]}

# Build the graph with explicit schemas
builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)
builder.add_edge(START, "answer_node")
builder.add_edge("answer_node", END)
graph = builder.compile()

# Run the graph
print(graph.invoke({"question": "hi"}))

---

Language: None
```bash
npm install @modelcontextprotocol/sdk
```

---

Language: None
> **Note**
> Replace `serverUrl` with your LangGraph server URL and configure authentication headers as needed.

---

Language: None
```js
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StreamableHTTPClientTransport } from "@modelcontextprotocol/sdk/client/streamableHttp.js";

---

Language: None
// Connects to the LangGraph MCP endpoint
async function connectClient(url) {
    const baseUrl = new URL(url);
    const client = new Client({
        name: 'streamable-http-client',
        version: '1.0.0'
    });

---

Language: None
    const transport = new StreamableHTTPClientTransport(baseUrl);
    await client.connect(transport);

---

Language: None
    console.log("Connected using Streamable HTTP transport");
    console.log(JSON.stringify(await client.listTools(), null, 2));
    return client;
}

---

Language: None
const serverUrl = "http://localhost:2024/mcp";

---

Language: None
connectClient(serverUrl)
    .then(() => {
        console.log("Client connected successfully");
    })
    .catch(error => {
        console.error("Failed to connect client:", error);
    });
```

---

Language: None
Install the adapter with:

---

Language: None
```bash
pip install langchain-mcp-adapters
```

---

Language: None
Here is an example of how to connect to a remote MCP endpoint and use an agent as a tool:

---

Language: None
```python
# Create server parameters for stdio connection
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client
import asyncio

---

Language: None
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent

---

Language: None
server_params = {
    "url": "https://mcp-finance-agent.xxx.us.langgraph.app/mcp",
    "headers": {
        "X-Api-Key":"lsv2_pt_your_api_key"
    }
}

---

Language: None
async def main():
    async with streamablehttp_client(**server_params) as (read, write, _):
        async with ClientSession(read, write) as session:
            # Initialize the connection
            await session.initialize()

---

Language: None
            # Load the remote graph as if it was a tool
            tools = await load_mcp_tools(session)

---

Language: None
            # Create and run a react agent with the tools
            agent = create_react_agent("openai:gpt-4.1", tools)

---

Language: None
            # Invoke the agent with a message
            agent_response = await agent.ainvoke({"messages": "What can the finance agent do for me?"})
            print(agent_response)

---

Language: None
if __name__ == "__main__":
    asyncio.run(main())
```

---

Language: json
{
  "http": {
    "disable_mcp": true
  }
}

---

