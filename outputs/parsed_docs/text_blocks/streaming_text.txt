[LangGraph SDK](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/) allows you to stream outputs from the LangGraph API server.

Basic usage example:

=== "Python"

=== "JavaScript"

=== "cURL"

??? example "Extended example: streaming updates"

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

=== "Python"

=== "JavaScript"

=== "cURL"

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

!!! note "Stateful runs"

=== "updates"

===  "values"

To include outputs from [subgraphs](../../concepts/subgraphs.md) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

=== "Python"

=== "JavaScript"

=== "cURL"

Use the `messages-tuple` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages-tuple` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

=== "Python"

=== "JavaScript"

=== "cURL"

To send **custom user-defined data**:

=== "Python"

=== "JavaScript"

=== "cURL"

To stream all events, including the state of the graph:

=== "Python"

=== "JavaScript"

=== "cURL"

