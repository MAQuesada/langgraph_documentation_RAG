# LangGraph RAG Pipeline: Retrieval-Augmented Generation over Technical Documentation

## ğŸ“Œ Abstract

Retrieval-Augmented Generation (RAG) has emerged as a transformative technique in the domain of natural language processing, enabling large language models (LLMs) to access and reason over external knowledge sources without the need for constant retraining. This hybrid framework combines the strengths of dense retrieval and generative modeling, allowing systems to dynamically incorporate relevant context from large knowledge base such as technical documentation into their responses. In the context of software and framework documentation, RAG significantly improves accessibility by allowing users to interact with complex materials through natural language queries. Instead of manually searching through static documents, developers and users can receive context-aware, conversational answers tailored to their information needs. This capability makes RAG an invaluable tool for enhancing developer productivity, reducing onboarding time, and democratizing access to specialized technical knowledge.

---

## ğŸ” Introduction

LangGraph Documentation RAG is an advanced Retrieval-Augmented Generation (RAG) pipeline designed to make the official LangGraph documentation easily searchable, chunkable, and ready for LLM-powered question answering. The project clones the latest LangGraph documentation, parses and organizes markdown and notebook files, extracts code and text blocks, and stores them in a vector database (Qdrant). Seamless integration of modern text chunking, metadata enrichment, and support for OpenAI embeddings enables powerful semantic search and Q&A functionality over the LangGraph documentation. 

**Main frameworks used:**
- **LangGraph**: for building end-to-end RAG pipeline.
- **LangChain**: for LLM abstraction and orchestration.
- **Qdrant**: as a scalable vector database.
- **OpenAI**: for embeddings and language generation.

---

## ğŸ—ï¸ System Architecture and steps for LangGraph documentation RAG workflow

### 1. ğŸ“š Documentation Loading and Ingestion

LangGraph documentation is cloned from its public repository using a custom ingestion pipeline. The process includes:

- Cleaning old outputs for upgrading with new uploaded documentation if available.
- Loading all markdown and text files from the directory if downloading from github not needed.
- Converting .ipynb files into markdown string using only markdown and code cells while ignoring output cells for better data to generate semantic embeddings.
- Recursive chunking with a maximum chunk size limit.
- Saving processed chunks for further processing and generate embeddings.
- Embedding text chunks using OpenAIâ€™s `text-embedding-3-large` model and ingested into Qdrant database with appropriate metadata.
  
#### Documentation descriptive statistics

**Total number of documents:** 184
**Minimum of document length:** 264
**Average of document length:** 10345.88
**Maximum of document length:** 88210
**Q1 (25th percentile):** 3294.5
**Values below Q1:** 46
**Q2 (50th percentile):** 7471.0
**Values below Q2:** 92
**Q3 (75th percentile):** 13031.5
**Values below Q3:** 138



### 2. ğŸ” Populating the Vector Store (Qdrant)

- Qdrant vector database is chosen for its efficient similarity search, REST API, local persistence, and production-readiness.
- Indexed chunks are embedded using OpenAIâ€™s embedding model and stored with contextual metadata (chunk id, content, file path).
- Search queries are performed using cosine similarity to fetch the top-k relevant chunks per user input.
- Qdrant Vector Store instance is used along with caching so that the vector store is only created once.


### 3. ğŸ§  Prompting Workflow

- Loading prompts based on a YAML config file.
- Formatting and building prompts based on section intro, content, and config dictionaries.
- Configuration includes information like role of the system, goal to be achieved, instruction given to the LLM, background information as context, output constraints, output format, examples, and style or tone
  among others.
- Reasoning strategies including Chain of Thought (CoT), Self Ask, and ReAct are mentioned in the relevant config file to be used as found appropriate. 



### 4. ğŸ¤– RAG Pipeline with LangGraph
LangGraph powers the logic of the agentâ€™s interaction:

â€¢Each user query becomes a new workflow in LangGraph.

â€¢Nodes represent tool calls (retriever, prompt executor, memory manager).

â€¢The Retriever Node fetches context from Qdrant.

â€¢The LLM Node decides whether to:

â€¢Answer with current context

â€¢Ask sub-questions and fetch more info

â€¢Store conversation in memory (max 5 turns)

â€¢LangGraph also manages branching logic and agent state transitions with full traceability using LangSmith.

### 5. ğŸ’¬ Memory and Context Handling

We integrate LangGraphâ€™s native memory system using PostgreSQL checkpoints. This ensures:

â€¢Long-running conversations retain user context

â€¢State is restored across sessions

â€¢Trimming to 5 past interactions maintains focus and prevents bloating

### 6. ğŸ§ª Monitoring and Evaluation
LangSmith is used for:

â€¢Observing LLM inputs/outputs

â€¢Visualizing tool calls and retrieval paths

â€¢Measuring latency, response length, and accuracy

â€¢Conducting offline evaluation with saved traces

### 7. ğŸ’» Example Use Cases
â€œWhat is the difference between a workflow and an agent in LangGraph?â€
â†’ The assistant breaks down the question into two tool calls, retrieves definitions from docs, and synthesizes a clear answer.

â€œHow does checkpointing work in LangGraph?â€
â†’ The assistant navigates to the memory documentation, retrieves steps, and formats a detailed guide.

â€œExplain how multi-agent routing is handled in LangGraph.â€
â†’ The assistant fetches multiple docs, compares routing strategies, and provides examples from the codebase.

### 8. âœ… Observations:
ğŸ’ª Strengths
â€¢Modular and well-structured RAG implementation

â€¢Agent can handle multi-hop technical queries

â€¢Stack uses only open-source components and scalable tools

### 9. âš ï¸ Limitations
â€¢Retrieval depends heavily on chunking quality

â€¢Latency increases with decomposition and multi-hop retrieval

â€¢Lacks reranking or cross-encoder scoring for better chunk selection

### 10. ğŸ§© Conclusion
This project demonstrates the power of LangGraph as both a framework and knowledge source in building domain-specific RAG agents. The assistant provides reliable, traceable, and explainable responses on technical documentationâ€”paving the way for customizable support agents across any domain.

### ğŸš€ Future Work
â€¢Integrate reranking via LLM or cross-encoder

â€¢Summarize long documents pre-indexing

â€¢Add user personalization to memory and retrieval

### ğŸ‘¥ Contributors
â€¢Manuel â€“ https://www.linkedin.com/in/manuel-alejandro-quesada-martinez-0b9534238/

â€¢Utkarsh â€“ https://www.linkedin.com/in/utkarsh-dubey-a87a71209/

â€¢Pranav â€“ https://www.linkedin.com/in/pranav-tiwari-122706249/

ğŸ“« Contact
Open to collaboration and contributions:
ğŸ“§ Email: [mailto:tu_email@ejemplo.com, tiwari.pranav1999@gmail.com, utkarsh251096@gmail.com]
ğŸŒ GitHub: [tu\_usuario](https://github.com/tu_usuario)

